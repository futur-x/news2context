"""
æ–°é—»é‡‡é›†æ ¸å¿ƒé€»è¾‘
"""

import asyncio
import aiohttp
from datetime import datetime, date
from typing import List, Dict, Any
from datetime import datetime
from loguru import logger

from src.utils.markdown_parser import MarkdownParser
from src.utils.chunker import SmartChunker

from src.core.task_manager import TaskManager
from src.storage.weaviate_client import CollectionManager
from src.engines.factory import EngineFactory
from src.extractor import ContentExtractor
from src.utils.config import get_config

class NewsCollector:
    """æ–°é—»é‡‡é›†å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.task_manager = TaskManager()
        self.collection_manager = CollectionManager(
            weaviate_url=self.config.get('weaviate.url'),
            api_key=self.config.get('weaviate.api_key')
        )
        self.engine = EngineFactory.create_engine(self.config.config)
        self.extractor = ContentExtractor()

    async def collect_task(self, task_name: str) -> int:
        """
        æ‰§è¡ŒæŒ‡å®šä»»åŠ¡çš„é‡‡é›†
        
        Args:
            task_name: ä»»åŠ¡åç§°
            
        Returns:
            é‡‡é›†æ•°é‡
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œé‡‡é›†ä»»åŠ¡: {task_name}")
        
        task = self.task_manager.get_task(task_name)
        if not task:
            logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_name}")
            return 0
            
        # ç¡®ä¿ Collection å­˜åœ¨
        collection_name = task.weaviate['collection']
        if not self.collection_manager.collection_exists(collection_name):
            logger.info(f"åˆ›å»º Collection: {collection_name}")
            self.collection_manager.create_collection(collection_name)
            
        # ä»é…ç½®è¯»å–å‚æ•°
        max_news_per_source = self.config.get('collector.max_news_per_source', 15)
        early_stop_threshold = self.config.get('collector.early_stop_threshold', 3)
        
        logger.info(f"é‡‡é›†é…ç½®: æ¯æºæœ€å¤š {max_news_per_source} æ¡, è¿ç»­å¤±è´¥ {early_stop_threshold} æ¬¡è·³è¿‡")
        
        # ç”¨äºæš‚å­˜æ‰€æœ‰é‡‡é›†çš„æ–°é—»
        all_news_items = []
        source_stats = {}  # ç»Ÿè®¡æ¯ä¸ªæºçš„é‡‡é›†æƒ…å†µ
        
        logger.info(f"å¼€å§‹é‡‡é›†æœ€æ–°æ–°é—»ï¼ˆå…± {len(task.sources)} ä¸ªæºï¼‰")
        
        async with aiohttp.ClientSession() as session:
            for source in task.sources:
                source_name = source['name']
                source_stats[source_name] = {'total': 0, 'success': 0, 'failed': 0, 'skipped': 0}
                
                try:
                    # è·å–æœ€æ–°æ–°é—»åˆ—è¡¨
                    news_items = await self.engine.fetch_news(source['hashid'])
                    
                    if not news_items:
                        logger.warning(f"æº {source_name} æ²¡æœ‰æ–°é—»æ•°æ®ï¼Œè·³è¿‡")
                        continue
                    
                    # é™åˆ¶æ¯ä¸ªæºçš„æ–°é—»æ•°é‡
                    original_count = len(news_items)
                    news_items = news_items[:max_news_per_source]
                    
                    source_stats[source_name]['total'] = len(news_items)
                    
                    if original_count > max_news_per_source:
                        logger.info(
                            f"æº {source_name} åŸæœ‰ {original_count} æ¡æ–°é—»ï¼Œ"
                            f"é™åˆ¶ä¸º {max_news_per_source} æ¡"
                        )
                    else:
                        logger.info(f"æº {source_name} è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
                        
                    # å¤„ç†æ¯æ¡æ–°é—»ï¼Œå¸¦æ—©åœæœºåˆ¶
                    consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°
                    
                    for idx, item in enumerate(news_items, 1):
                        # æ—©åœæ£€æŸ¥
                        if consecutive_failures >= early_stop_threshold:
                            skipped = len(news_items) - idx + 1
                            source_stats[source_name]['skipped'] = skipped
                            logger.warning(
                                f"æº {source_name} è¿ç»­ {early_stop_threshold} æ¡æå–å¤±è´¥ï¼Œ"
                                f"è·³è¿‡å‰©ä½™ {skipped} æ¡æ–°é—»"
                            )
                            break
                        
                        # æå–æ­£æ–‡
                        content_data = await self.extractor.extract_from_url(item['url'], session)
                        
                        if not content_data:
                            # æå–å¤±è´¥ï¼Œå¢åŠ å¤±è´¥è®¡æ•°
                            consecutive_failures += 1
                            source_stats[source_name]['failed'] += 1
                            logger.debug(f"ç¬¬ {idx} æ¡æå–å¤±è´¥ï¼Œè¿ç»­å¤±è´¥: {consecutive_failures}/{early_stop_threshold}")
                            continue
                        
                        # æå–æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                        consecutive_failures = 0
                        source_stats[source_name]['success'] += 1
                        
                        # æ ¼å¼åŒ–æ—¶é—´ (RFC3339)
                        current_time = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
                        
                        news_data = {
                            "task_name": task_name,
                            "title": item['title'],
                            "content": content_data.get('content', '') if content_data else item.get('excerpt', ''),
                            "url": item['url'],
                            "source_name": source_name,
                            "source_hashid": source['hashid'],
                            "published_at": current_time,
                            "fetched_at": current_time,
                            "category": source.get('category', 'ç»¼åˆ'),
                            "excerpt": item.get('excerpt', '')
                        }
                        
                        all_news_items.append(news_data)
                    
                    logger.info(
                        f"æº {source_name} å®Œæˆ: "
                        f"æˆåŠŸ {source_stats[source_name]['success']} æ¡, "
                        f"å¤±è´¥ {source_stats[source_name]['failed']} æ¡, "
                        f"è·³è¿‡ {source_stats[source_name]['skipped']} æ¡"
                    )
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æº {source_name} å¤±è´¥: {str(e)}")
        
        # 5. ç”Ÿæˆ Markdown æ‘˜è¦æ–‡ä»¶
        logger.info("æ­£åœ¨ç”Ÿæˆ Markdown æ‘˜è¦æ–‡ä»¶...")
        markdown_path = self._generate_markdown_digest(task_name, all_news_items, source_stats)
        logger.success(f"Markdown æ‘˜è¦å·²ç”Ÿæˆ: {markdown_path}")
        
        # 6. æ™ºèƒ½åˆ‡å‰²æˆ chunks
        if self.config.get('weaviate.chunking.enabled', True):
            logger.info("å¼€å§‹æ™ºèƒ½åˆ‡å‰²...")
            
            # è§£æ Markdownï¼Œæå–æ–‡ç« 
            parser = MarkdownParser()
            articles = parser.parse_digest(markdown_path)
            
            # æ™ºèƒ½åˆ‡å‰²æˆ chunks
            max_chunk_size = self.config.get('weaviate.chunking.max_chunk_size', 3000)
            chunker = SmartChunker(max_chunk_size=max_chunk_size)
            chunks = chunker.create_chunks(articles)
            
            logger.info(f"åˆ‡å‰²å®Œæˆ: {len(articles)} ç¯‡æ–°é—» â†’ {len(chunks)} ä¸ª chunks")
            
            # å‡†å¤‡ chunk æ•°æ®
            chunk_data_list = []
            for chunk in chunks:
                chunk_data = {
                    "chunk_id": chunk.chunk_id,
                    "content": chunk.content,
                    "task_name": task_name,
                    "categories": chunk.metadata["categories"],
                    "sources": chunk.metadata["sources"],
                    "article_titles": chunk.metadata["article_titles"],
                    "article_count": chunk.metadata["article_count"],
                    "char_count": chunk.metadata["char_count"],
                    "created_at": datetime.now().isoformat() + "Z"
                }
                chunk_data_list.append(chunk_data)
            
            # æ‰¹é‡æ’å…¥ chunks
            batch_size = self.config.get('weaviate.batch.size', 5)
            total_chunks = self.collection_manager.batch_insert_chunks(
                collection_name=collection_name,
                chunks=chunk_data_list,
                batch_size=batch_size
            )
            
            logger.success(f"âœ“ æˆåŠŸå…¥åº“ {total_chunks} ä¸ª chunks")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.task_manager.update_task_status(
                task_name,
                last_run=datetime.now()
            )
            logger.success(f"ä»»åŠ¡ {task_name} é‡‡é›†å®Œæˆï¼Œå…±å…¥åº“ {total_chunks} ä¸ª chunks")
            return total_chunks
        else:
            # é™çº§æ–¹æ¡ˆï¼šç›´æ¥æ’å…¥æ–°é—»ï¼ˆæ—§é€»è¾‘ï¼‰
            logger.warning("æ™ºèƒ½åˆ‡å‰²å·²ç¦ç”¨ï¼Œä½¿ç”¨æ—§çš„ç›´æ¥æ’å…¥æ–¹å¼")
            total_news = self.collection_manager.batch_insert_news(collection_name, all_news_items)
            logger.success(f"âœ“ æˆåŠŸå…¥åº“ {total_news} æ¡æ–°é—»")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.task_manager.update_task_status(
                task_name,
                last_run=datetime.now()
            )
            logger.success(f"ä»»åŠ¡ {task_name} é‡‡é›†å®Œæˆï¼Œå…±å…¥åº“ {total_news} æ¡æ–°é—»")
            return total_news
    
    def _parse_date_range(self, date_range: str, custom_range: Dict[str, Any] = None) -> List[date]:
        """
        è§£ææ—¥æœŸèŒƒå›´é…ç½®
        
        Args:
            date_range: æ—¥æœŸèŒƒå›´ç±»å‹ (today, yesterday, last_3_days, last_7_days, custom)
            custom_range: è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´ {'start': 'YYYY-MM-DD', 'end': 'YYYY-MM-DD'}
            
        Returns:
            æ—¥æœŸåˆ—è¡¨
        """
        from datetime import timedelta
        
        today = date.today()
        
        # TopHub API ä»…æ”¯æŒæ˜¨å¤©åŠä»¥å‰çš„æ•°æ®
        if date_range == 'yesterday' or date_range == 'last_1_days':
            # æœ€è¿‘1å¤© = æ˜¨å¤©
            return [today - timedelta(days=1)]
    
    def _generate_markdown_digest(
        self, 
        task_name: str, 
        news_items: List[Dict[str, Any]], 
        source_stats: Dict[str, Dict[str, int]]
    ) -> None:
        """
        ç”Ÿæˆæ–°é—»æ‘˜è¦ Markdown æ–‡ä»¶
        
        Args:
            task_name: ä»»åŠ¡åç§°
            news_items: æ‰€æœ‰é‡‡é›†çš„æ–°é—»åˆ—è¡¨
            source_stats: æ¯ä¸ªæºçš„ç»Ÿè®¡ä¿¡æ¯
        """
        from pathlib import Path
        from collections import defaultdict
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        today = datetime.now().strftime('%Y-%m-%d')
        output_dir = Path('output') / today
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f'{task_name}_digest.md'
        
        # æŒ‰åˆ†ç±»å’Œæ¥æºç»„ç»‡æ–°é—»
        news_by_category = defaultdict(lambda: defaultdict(list))
        for item in news_items:
            category = item.get('category', 'å…¶ä»–')
            source = item.get('source_name', 'æœªçŸ¥æ¥æº')
            news_by_category[category][source].append(item)
        
        # ç”Ÿæˆ Markdown å†…å®¹
        lines = []
        lines.append(f"# {task_name} æ–°é—»æ‘˜è¦ - {today}\n")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        lines.append(f"**æ€»è®¡**: {len(news_items)} æ¡æ–°é—»\n")
        lines.append("---\n")
        
        # ç”Ÿæˆç›®å½•
        lines.append("## ğŸ“‘ ç›®å½•\n")
        for category in sorted(news_by_category.keys()):
            lines.append(f"- [{category}](#{category.replace(' ', '-')})")
        lines.append("\n---\n")
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        lines.append("## ğŸ“Š é‡‡é›†ç»Ÿè®¡\n")
        lines.append("| æ–°é—»æº | è·å– | æˆåŠŸ | å¤±è´¥ | è·³è¿‡ |")
        lines.append("|--------|------|------|------|------|")
        for source, stats in source_stats.items():
            lines.append(
                f"| {source} | {stats['total']} | {stats['success']} | "
                f"{stats['failed']} | {stats['skipped']} |"
            )
        lines.append("\n---\n")
        
        # ç”Ÿæˆå„åˆ†ç±»çš„æ–°é—»å†…å®¹
        for category in sorted(news_by_category.keys()):
            lines.append(f"## {category}\n")
            
            for source in sorted(news_by_category[category].keys()):
                news_list = news_by_category[category][source]
                lines.append(f"### ğŸ“° {source}\n")
                lines.append(f"**æ–‡ç« æ•°é‡**: {len(news_list)}\n")
                
                for idx, news in enumerate(news_list, 1):
                    lines.append(f"#### {idx}. {news['title']}\n")
                    lines.append(f"**åŸæ–‡é“¾æ¥**: [{news['url']}]({news['url']})\n")
                    
                    if news.get('content'):
                        lines.append("**æ­£æ–‡å†…å®¹**:\n")
                        lines.append(f"{news['content']}\n")
                    elif news.get('excerpt'):
                        lines.append("**æ‘˜è¦**:\n")
                        lines.append(f"{news['excerpt']}\n")
                    
                    lines.append("---\n")
        
        # ä¿å­˜æ–‡ä»¶
        content = '\n'.join(lines)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.success(f"æ–°é—»æ‘˜è¦å·²ä¿å­˜åˆ°: {output_file}")
        return str(output_file)  # è¿”å›æ–‡ä»¶è·¯å¾„
