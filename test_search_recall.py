"""
é›†æˆæµ‹è¯•ï¼šéªŒè¯æœç´¢å¬å›åŠŸèƒ½
æµ‹è¯•åœºæ™¯ï¼šæœç´¢åˆ°æŸä¸ª chunk åï¼Œèƒ½å¦å¬å›å¹¶æ‹¼æ¥æ•´ç¯‡æ–‡ç« çš„æ‰€æœ‰ chunks
"""

import sys
from pathlib import Path
from datetime import datetime

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.chunker import ArticleChunker
from src.utils.markdown_parser import Article
from src.storage.weaviate_client import CollectionManager
from src.utils.config import get_config


def test_search_recall():
    """æµ‹è¯•æœç´¢å¬å›ï¼šæœåˆ°æŸä¸ª chunkï¼Œè¿”å›å®Œæ•´æ–‡ç« """
    print("\n" + "="*60)
    print("é›†æˆæµ‹è¯•ï¼šæœç´¢å¬å›ä¸æ‹¼æ¥åŠŸèƒ½")
    print("="*60)

    # åˆå§‹åŒ–
    config = get_config()

    # è·å– Embedding API Key
    embedding_api_key = config.get('embedding.api_key') or config.get('llm.api_key')
    headers = {}
    if embedding_api_key:
        headers["X-OpenAI-Api-Key"] = embedding_api_key

    # å‡†å¤‡ embedding é…ç½®
    embedding_config = {
        'model': config.get('embedding.model', 'text-embedding-3-small'),
        'base_url': config.get('embedding.base_url', 'https://litellm.futurx.cc'),
        'dimensions': config.get('embedding.dimensions', 1536)
    }

    try:
        collection_manager = CollectionManager(
            weaviate_url=config.get('weaviate.url'),
            api_key=config.get('weaviate.api_key'),
            additional_headers=headers,
            embedding_config=embedding_config
        )
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Weaviate: {e}")
        print("   è¯·ç¡®ä¿ Weaviate æœåŠ¡æ­£åœ¨è¿è¡Œï¼šdocker-compose up -d")
        return False

    # æµ‹è¯• collection åç§°
    test_collection = "TestSearchRecall"

    try:
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§æµ‹è¯•æ•°æ®
        if collection_manager.collection_exists(test_collection):
            print(f"\næ¸…ç†æ—§æµ‹è¯•æ•°æ®: {test_collection}")
            collection_manager.delete_collection(test_collection)

        # åˆ›å»ºæµ‹è¯• collectionï¼ˆä½¿ç”¨ NewsChunk schemaï¼‰
        print(f"\nåˆ›å»ºæµ‹è¯• collection: {test_collection}")
        collection_manager.create_collection(
            test_collection,
            collection_manager.NEWS_CHUNK_SCHEMA
        )

        # === ç¬¬ 1 æ­¥ï¼šå‡†å¤‡æµ‹è¯•æ•°æ® ===
        print("\n=== ç¬¬ 1 æ­¥ï¼šå‡†å¤‡æµ‹è¯•æ•°æ® ===")

        # åˆ›å»ºä¸€ç¯‡é•¿æ–‡ç« ï¼ŒåŒ…å«ç‰¹å®šå…³é”®è¯åœ¨ä¸åŒä½ç½®
        chunker = ArticleChunker(max_tokens=1000)  # è®¾ç½®è¾ƒå°çš„å€¼ç¡®ä¿åˆ‡å‰²

        # æ–‡ç« å†…å®¹ï¼šåœ¨ç¬¬ 1ã€3ã€5 éƒ¨åˆ†åŒ…å«ä¸åŒçš„å…³é”®è¯
        part1 = "ç¬¬ä¸€éƒ¨åˆ†å†…å®¹ï¼šä»‹ç»äººå·¥æ™ºèƒ½çš„åŸºç¡€æ¦‚å¿µã€‚" + ("AIæŠ€æœ¯å‘å±•è¿…é€Ÿã€‚" * 100)
        part2 = "ç¬¬äºŒéƒ¨åˆ†å†…å®¹ï¼šæ·±å…¥æ¢è®¨æœºå™¨å­¦ä¹ ç®—æ³•ã€‚" + ("ä¼ ç»Ÿç®—æ³•é¢ä¸´æŒ‘æˆ˜ã€‚" * 100)
        part3 = "ç¬¬ä¸‰éƒ¨åˆ†å†…å®¹ï¼šè®¨è®ºæ·±åº¦å­¦ä¹ çš„çªç ´ã€‚" + ("ç¥ç»ç½‘ç»œæ€§èƒ½ä¼˜å¼‚ã€‚" * 100)
        part4 = "ç¬¬å››éƒ¨åˆ†å†…å®¹ï¼šåˆ†æè‡ªç„¶è¯­è¨€å¤„ç†è¿›å±•ã€‚" + ("NLPåº”ç”¨å¹¿æ³›ã€‚" * 100)
        part5 = "ç¬¬äº”éƒ¨åˆ†å†…å®¹ï¼šå±•æœ›é‡å­è®¡ç®—çš„æœªæ¥ã€‚" + ("é‡å­ä¼˜åŠ¿æ˜æ˜¾ã€‚" * 100)

        long_content = "\n\n".join([part1, part2, part3, part4, part5])

        article = Article(
            title="AIæŠ€æœ¯å…¨é¢è§£æ",
            source="ç§‘æŠ€æ—¥æŠ¥",
            category="ç§‘æŠ€",
            url="https://test.com/ai-analysis",
            content=long_content,
            char_count=len(long_content)
        )

        # åˆ‡å‰²æˆ chunks
        chunks = chunker.chunk_articles([article], task_name="test_search")

        print(f"æ–‡ç« è¢«åˆ‡å‰²æˆ: {len(chunks)} ä¸ª chunks")

        # æ£€æŸ¥æ¯ä¸ª chunk çš„å†…å®¹
        for i, chunk in enumerate(chunks):
            content_preview = chunk.content[:100].replace('\n', ' ')
            print(f"   Chunk {i}: {content_preview}...")

        # === ç¬¬ 2 æ­¥ï¼šæ’å…¥æ•°æ®åˆ° Weaviate ===
        print(f"\n=== ç¬¬ 2 æ­¥ï¼šæ’å…¥ {len(chunks)} ä¸ª chunks åˆ° Weaviate ===")

        chunk_data_list = []
        for chunk in chunks:
            chunk_data = {
                "article_id": chunk.article_id,
                "chunk_index": chunk.chunk_index,
                "total_chunks": chunk.total_chunks,
                "title": chunk.title,
                "content": chunk.content,
                "url": chunk.url,
                "source_name": chunk.source_name,
                "source_hashid": chunk.source_hashid,
                "category": chunk.category,
                "published_at": datetime.now().isoformat() + "Z",
                "fetched_at": datetime.now().isoformat() + "Z",
                "task_name": chunk.task_name,
                "excerpt": chunk.excerpt
            }
            chunk_data_list.append(chunk_data)

        inserted_count = collection_manager.batch_insert_chunks(
            collection_name=test_collection,
            chunks=chunk_data_list,
            batch_size=5
        )

        print(f"âœ“ æˆåŠŸæ’å…¥: {inserted_count} ä¸ª chunks")

        # ç­‰å¾… Weaviate ç´¢å¼•
        import time
        print("ç­‰å¾… Weaviate ç´¢å¼•...")
        time.sleep(2)

        # === ç¬¬ 3 æ­¥ï¼šæµ‹è¯•ä¸åŒçš„æœç´¢å…³é”®è¯ ===
        print("\n=== ç¬¬ 3 æ­¥ï¼šæµ‹è¯•æœç´¢å¬å› ===")

        test_queries = [
            ("ç¥ç»ç½‘ç»œ", "åº”è¯¥åœ¨ç¬¬ 3 éƒ¨åˆ†"),
            ("é‡å­è®¡ç®—", "åº”è¯¥åœ¨ç¬¬ 5 éƒ¨åˆ†"),
            ("åŸºç¡€æ¦‚å¿µ", "åº”è¯¥åœ¨ç¬¬ 1 éƒ¨åˆ†"),
        ]

        all_passed = True

        for query, expected_location in test_queries:
            print(f"\nğŸ” æœç´¢: '{query}' ({expected_location})")

            # ä½¿ç”¨ç»Ÿä¸€æœç´¢æ¥å£
            results = collection_manager.unified_search(
                collection_name=test_collection,
                query=query,
                limit=5
            )

            if not results:
                print(f"   âŒ æ²¡æœ‰æœç´¢åˆ°ä»»ä½•ç»“æœ")
                all_passed = False
                continue

            # æ£€æŸ¥ç¬¬ä¸€ä¸ªç»“æœ
            result = results[0]

            print(f"   âœ“ æ‰¾åˆ°ç»“æœ:")
            print(f"     - æ ‡é¢˜: {result.get('title')}")
            print(f"     - Article ID: {result.get('id')}")
            print(f"     - Chunks æ•°é‡: {result.get('_additional', {}).get('chunk_count', 'N/A')}")
            print(f"     - å†…å®¹é•¿åº¦: {len(result.get('content', ''))} å­—ç¬¦")
            print(f"     - ç›¸å…³åº¦: {result.get('_additional', {}).get('certainty', 0):.3f}")

            # éªŒè¯å†…å®¹å®Œæ•´æ€§
            content = result.get('content', '')

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰éƒ¨åˆ†çš„å…³é”®è¯ï¼ˆè¯æ˜æ‹¼æ¥äº†æ‰€æœ‰ chunksï¼‰
            expected_keywords = [
                "ç¬¬ä¸€éƒ¨åˆ†",
                "ç¬¬äºŒéƒ¨åˆ†",
                "ç¬¬ä¸‰éƒ¨åˆ†",
                "ç¬¬å››éƒ¨åˆ†",
                "ç¬¬äº”éƒ¨åˆ†"
            ]

            found_keywords = [kw for kw in expected_keywords if kw in content]

            if len(found_keywords) == len(expected_keywords):
                print(f"   âœ… éªŒè¯é€šè¿‡: åŒ…å«æ‰€æœ‰ {len(expected_keywords)} ä¸ªéƒ¨åˆ†ï¼ˆå®Œæ•´æ‹¼æ¥ï¼‰")
            else:
                print(f"   âŒ éªŒè¯å¤±è´¥: åªæ‰¾åˆ° {len(found_keywords)}/{len(expected_keywords)} ä¸ªéƒ¨åˆ†")
                print(f"      ç¼ºå¤±: {set(expected_keywords) - set(found_keywords)}")
                all_passed = False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœç´¢å…³é”®è¯
            if query in content:
                print(f"   âœ… åŒ…å«æœç´¢å…³é”®è¯: '{query}'")
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°æœç´¢å…³é”®è¯: '{query}'")

        # === ç¬¬ 4 æ­¥ï¼šéªŒè¯ article_id å…³è” ===
        print(f"\n=== ç¬¬ 4 æ­¥ï¼šéªŒè¯ article_id å…³è” ===")

        # ç›´æ¥æŸ¥è¯¢æ‰€æœ‰ chunks
        all_chunks_query = collection_manager.client.query.get(
            test_collection,
            ["article_id", "chunk_index", "total_chunks", "title"]
        ).with_limit(100).do()

        all_chunks_data = all_chunks_query['data']['Get'].get(test_collection, [])

        # æŒ‰ article_id åˆ†ç»„
        from collections import defaultdict
        articles_map = defaultdict(list)

        for chunk_data in all_chunks_data:
            article_id = chunk_data.get('article_id')
            articles_map[article_id].append(chunk_data)

        print(f"æ•°æ®åº“ä¸­çš„æ–‡ç« æ•°: {len(articles_map)}")

        for article_id, article_chunks in articles_map.items():
            print(f"\n   Article ID: {article_id}")
            print(f"     - æ ‡é¢˜: {article_chunks[0].get('title')}")
            print(f"     - Chunks æ•°é‡: {len(article_chunks)}")
            print(f"     - Total chunks (å£°æ˜): {article_chunks[0].get('total_chunks')}")

            # éªŒè¯ chunk_index è¿ç»­
            indices = sorted([c.get('chunk_index') for c in article_chunks])
            expected_indices = list(range(len(article_chunks)))

            if indices == expected_indices:
                print(f"     âœ… Chunk ç´¢å¼•è¿ç»­: {indices}")
            else:
                print(f"     âŒ Chunk ç´¢å¼•ä¸è¿ç»­: {indices}")
                all_passed = False

        # === æ¸…ç†æµ‹è¯•æ•°æ® ===
        print(f"\n=== æ¸…ç†æµ‹è¯•æ•°æ® ===")
        collection_manager.delete_collection(test_collection)
        print(f"âœ“ å·²åˆ é™¤æµ‹è¯• collection: {test_collection}")

        # === æµ‹è¯•ç»“æœ ===
        print("\n" + "="*60)
        if all_passed:
            print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æœç´¢å¬å›åŠŸèƒ½æ­£å¸¸ã€‚")
        else:
            print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯ã€‚")
        print("="*60)

        return all_passed

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

        # æ¸…ç†æµ‹è¯•æ•°æ®
        try:
            if collection_manager.collection_exists(test_collection):
                collection_manager.delete_collection(test_collection)
        except:
            pass

        return False


if __name__ == "__main__":
    success = test_search_recall()
    sys.exit(0 if success else 1)
